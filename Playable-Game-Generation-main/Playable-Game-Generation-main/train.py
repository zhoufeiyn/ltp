# 0920 update: try to overfit level1-1 in one directory
import math
import os
import time
import cv2
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple, Optional
import re
from network.df.models.vae.autoencoder import AutoencoderKL
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms
from torchvision.utils import save_image
import configTrain as cfg
import matplotlib.pyplot as plt
from PIL import Image
import imageio
from model import get_model, get_data, get_web_img

device: str = "cuda" if torch.cuda.is_available() else "cpu"
# -----------------------------
# Custom Dataset for Mario Data
# -----------------------------

class MarioDataset(Dataset):
    """load mario dataset __init__ action and img paths,
     __getitem__  will return image and corresponding action"""
    """up to date: 2025-09-20 only load all frames in one directory,
     return array ofimages and actions"""
    def __init__(self, data_path: str, image_size):
        self.data_path = data_path
        self.image_size = image_size
        self.image_files = [] # image files path (xxx.png)
        self.actions = [] # action (0-255)
        self._load_data()
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(0.5, 0.5),  # [-1, 1]
        ])
    def _load_data(self):
        """load all png files and corresponding actions"""
        print(f"ğŸ” data path is scanning: {self.data_path}")
        if not os.path.exists(self.data_path): 
            print(f"âŒ data path not found: {self.data_path}")
            return
        
        total_files = 0
        valid_files = 0

        for root, dirs, files in os.walk(self.data_path):
            if root == self.data_path:
                continue
            for file in files:
                if file.lower().endswith('.png'):
                    total_files += 1
                    file_path = os.path.join(root, file)
                    
                    # å°è¯•ä»æ–‡ä»¶åæå–åŠ¨ä½œ
                    action = self._extract_action_from_filename(file)
                    if action is not None:
                        self.image_files.append(file_path)
                        self.actions.append(action)
                        valid_files += 1
                    else:
                        print(f"âš ï¸ can't extract action from filename: {file}")

    def _extract_action_from_filename(self, filename: str) -> Optional[int]:
        """extract action from filename"""
        # æ–‡ä»¶åæ ¼å¼: Rafael_dp2a9j4i_e6_1-1_f1000_a20_2019-04-13_20-13-16.win.png
        pattern = r'_a(\d+)_'
        match = re.search(pattern, filename)
        if match:
            return [int(match.group(1))]
        return None
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        """get the data sample of the specified index"""
        if idx >= len(self.image_files):
            raise IndexError(f"Index {idx} out of range for dataset of size {len(self.image_files)}")
        
        # åŠ è½½å›¾åƒ
        image_path = self.image_files[idx]
        image = Image.open(image_path).convert('RGB')
        image = self.transform(image)
        
        # è·å–åŠ¨ä½œï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        action = self.actions[idx] if idx < len(self.actions) else 0
        
        return image, action

def map_Key_to_Action(key) -> int:
    """map key(s) to action based on SMB dataset encoding:
    A=128(jump), up=64(climb), left=32, B=16(run/fire), 
    start=8, right=4, down=2, select=1
    
    Args:
        key: str or list of str - single key or list of pressed keys
    
    Examples:
        map_Key_to_Action("r") -> 4 (right)
        map_Key_to_Action(["r", "f"]) -> 20 (right + B = running right)
        map_Key_to_Action(["r", "f", "j"]) -> 148 (right + B + A = running jump right)
    """
    # å¦‚æœè¾“å…¥æ˜¯å•ä¸ªé”®ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
    if isinstance(key, str):
        keys = [key]
    else:
        keys = key
    
    action = 0
    
    # éå†æ‰€æœ‰æŒ‰ä¸‹çš„é”®ï¼Œç´¯åŠ åŠ¨ä½œå€¼
    for k in keys:
        if k == "r" or k == "right" or k == "â†’":
            action += 4  # right
        elif k == "l" or k == "left" or k == "â†":
            action += 32  # left
        elif k == "j" or k == "a":
            action += 128  # A (jump)
        elif k == "up" or k == "â†‘":
            action += 64  # up (climb)
        elif k == "f" or k == "b":
            action += 16  # B (run/fire)
        elif k == "s":
            action += 8  # start
        elif k == "d" or k == "down" or k == "â†“":
            action += 2  # down
        elif k == "enter":
            action += 1  # select
    
    return action

def build_video_sequence(dataset, num_frames):
    """build video sequence from dataset"""
    total_samples = len(dataset)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
    if total_samples < num_frames:
        print(f"âŒ dataset not enough: need at least {num_frames} samples, but only {total_samples} samples")
        return
    
    # è®¡ç®—å¯ä»¥åˆ›å»ºå¤šå°‘ä¸ªå®Œæ•´çš„è§†é¢‘åºåˆ—
    num_videos = total_samples // num_frames
    print(f"dataset loaded: {total_samples} samples, construct {num_videos} complete video sequences, each video has {num_frames} frames")
    
    # å­˜å‚¨æ‰€æœ‰è§†é¢‘åºåˆ—çš„æ•°æ®
    all_video_images = []  # å­˜å‚¨æ‰€æœ‰è§†é¢‘çš„å›¾åƒ
    all_video_actions = []  # å­˜å‚¨æ‰€æœ‰è§†é¢‘çš„åŠ¨ä½œ
    all_video_nonterminals = []  # å­˜å‚¨æ‰€æœ‰è§†é¢‘çš„nonterminals
    
    # åˆ›å»ºå¤šä¸ªè§†é¢‘åºåˆ—
    for video_idx in range(num_videos):
        video_images = []  # å­˜å‚¨å½“å‰è§†é¢‘çš„8å¸§å›¾åƒ
        video_actions = []  # å­˜å‚¨å½“å‰è§†é¢‘çš„8ä¸ªåŠ¨ä½œ
        video_nonterminals = []  # å­˜å‚¨å½“å‰è§†é¢‘çš„8ä¸ªnonterminals
        
        # æ„å»ºå½“å‰è§†é¢‘åºåˆ—
        start_idx = video_idx * num_frames
        for frame_idx in range(num_frames):
            idx = start_idx + frame_idx
            image, action = dataset[idx]
            video_images.append(image)  # image shape: [3, 128, 128]
            video_actions.append(action[0])  # actionæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
            video_nonterminals.append(True)  # å…ˆéƒ½é»˜è®¤True
        

        
        # è½¬æ¢ä¸ºtensorå¹¶ç»„ç»‡æˆç›®æ ‡æ ¼å¼
        # [num_frames, channels, h, w] = [8, 3, 128, 128]
        images_tensor = torch.stack(video_images, dim=0)  # [8, 3, 128, 128]
        images_tensor = images_tensor.unsqueeze(0)  # [1, 8, 3, 128, 128]
        
        # [batch_size, num_frames, action_dim] = [1, 8, 1]
        actions_tensor = torch.tensor(video_actions, dtype=torch.long)  # [8]
        actions_tensor = actions_tensor.unsqueeze(0).unsqueeze(-1)  # [1, 8, 1]
        
        # [batch_size, num_frames] = [1, 8]
        nonterminals_tensor = torch.tensor(video_nonterminals, dtype=torch.bool)  # [8]
        nonterminals_tensor = nonterminals_tensor.unsqueeze(0)  # [1, 8]
        
        # æ·»åŠ åˆ°æ€»åˆ—è¡¨ä¸­
        all_video_images.append(images_tensor)
        all_video_actions.append(actions_tensor)
        all_video_nonterminals.append(nonterminals_tensor)
        
    
    # åˆå¹¶æ‰€æœ‰è§†é¢‘åºåˆ—æ•°æ®
    # æœ€ç»ˆæ ¼å¼: [num_videos, num_frames, channels, h, w]
    batch_data = [
        torch.cat(all_video_images, dim=0),  # [num_videos, 8, 3, 128, 128]
        torch.cat(all_video_actions, dim=0),  # [num_videos, 8, 1]
        torch.cat(all_video_nonterminals, dim=0)  # [num_videos, 8]
    ]
    print(f"1.build video sequence completed")
    print(f"   batch_data[0] (images): {batch_data[0].shape}")  # [num_videos, 8, 3, 128, 128]
    print(f"   batch_data[1] (actions): {batch_data[1].shape}")  # [num_videos, 8, 1]
    print(f"   batch_data[2] (nonterminals): {batch_data[2].shape}")  # [num_videos, 8]   
    return batch_data

def train():
    device_obj = torch.device(device)
    print(f"ğŸš€ device: {device_obj}")
    dataset = MarioDataset(cfg.data_path, cfg.image_size)

    # video sequence parameters
    num_frames = 4
    print("1.build video sequence")
    batch_data = build_video_sequence(dataset, num_frames)

    model = get_model()
    diffusion = model.df_model
    vae = model.vae  
    epochs, lr, batch_size = cfg.epochs, cfg.lr, cfg.batch_size
    opt = torch.optim.AdamW(diffusion.parameters(), lr)  # åªä¼˜åŒ–diffusionæ¨¡å‹

    print("2.VAE encoding images to latent space")
    # å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´: [batch_size, num_frames, 3, 128, 128] -> [batch_size, num_frames, 4, 32, 32]
    with torch.no_grad():
        batch_size_videos, num_frames, channels, h, w = batch_data[0].shape
        # é‡å¡‘ä¸º [batch_size * num_frames, 3, 128, 128] è¿›è¡Œæ‰¹é‡ç¼–ç 
        images_flat = batch_data[0].reshape(-1, channels, h, w).to(device)
        
        # VAEç¼–ç 
        latent_dist = vae.encode(images_flat)
        latent_images = latent_dist.sample()  # é‡‡æ ·æ½œåœ¨è¡¨ç¤º
        latent_images = latent_images * 0.18215  # ç¼©æ”¾å› å­
        
        # é‡å¡‘å› [batch_size, num_frames, 4, 32, 32]
        latent_images = latent_images.reshape(batch_size_videos, num_frames, -1, h, w)
        
        # æ›´æ–°batch_data[0]ä¸ºç¼–ç åçš„æ½œåœ¨è¡¨ç¤ºï¼Œä¿æŒåœ¨GPUä¸Š
        batch_data[0] = latent_images
        
        print(f"   VAE encoding completed: {batch_data[0].shape}")

    print("3.start training")
    num_videos = batch_data[0].shape[0]  # æ€»è§†é¢‘æ•°é‡
    
    for epoch in range(epochs):
        total_loss = 0
        
        # éå†æ‰€æœ‰è§†é¢‘åºåˆ—
        for i in range(0, num_videos, batch_size):
            # è·å–å½“å‰æ‰¹æ¬¡çš„æ•°æ®
            end_idx = min(i + batch_size, num_videos)
            current_batch = [
                batch_data[0][i:end_idx],  # images: [batch_size, num_frames, c, h, w] - å·²åœ¨GPUä¸Š
                batch_data[1][i:end_idx].to(device),  # actions: [batch_size, num_frames, action_dim]
                batch_data[2][i:end_idx].to(device)   # nonterminals: [batch_size, num_frames]
            ]
            
            # è®­ç»ƒæ­¥éª¤
            out_dict = diffusion.training_step(current_batch)
            loss = out_dict["loss"]
            
            
            # åå‘ä¼ æ’­
            opt.zero_grad()
            loss.backward()
            opt.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / (num_videos // batch_size + (1 if num_videos % batch_size else 0))
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}")
    print("Training completed!")


if __name__ == "__main__":
    train()