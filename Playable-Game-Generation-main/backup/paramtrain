2025-10-04 23:59:25,302 - INFO - init log: logs/training_log_20251004_23.log
üîç data path is scanning: ./datatrain
/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
loaded pretrained LPIPS loss from /content/ltp/Playable-Game-Generation-main/Playable-Game-Generation-main/network/df/models/vae/lpips/vgg.pth
üì• load pretrained checkpoint: ckpt/model_epoch1000_20251002_234359.pth
‚úÖ Checkpoint loaded successfullyÔºÅ
‚úÖ VAE already loadedÔºåVAE parameters has been frozen
df_model.init_z: torch.Size([32, 32, 32]), requires_grad=True
df_model.transition_model.model.dit.pos_embed: torch.Size([1, 256, 384]), requires_grad=False
df_model.transition_model.model.dit.x_embedder.proj.weight: torch.Size([384, 36, 2, 2]), requires_grad=True
df_model.transition_model.model.dit.x_embedder.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.0.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.1.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.2.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.3.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.4.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.5.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.6.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.7.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.8.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.9.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.10.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.attn.qkv.weight: torch.Size([1152, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.attn.qkv.bias: torch.Size([1152]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.attn.proj.weight: torch.Size([384, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.attn.proj.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.mlp.fc1.weight: torch.Size([1536, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.mlp.fc1.bias: torch.Size([1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.mlp.fc2.weight: torch.Size([384, 1536]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.mlp.fc2.bias: torch.Size([384]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.adaLN_modulation.1.weight: torch.Size([2304, 384]), requires_grad=True
df_model.transition_model.model.dit.blocks.11.adaLN_modulation.1.bias: torch.Size([2304]), requires_grad=True
df_model.transition_model.model.dit.final_layer.linear.weight: torch.Size([144, 384]), requires_grad=True
df_model.transition_model.model.dit.final_layer.linear.bias: torch.Size([144]), requires_grad=True
df_model.transition_model.model.dit.final_layer.adaLN_modulation.1.weight: torch.Size([768, 384]), requires_grad=True
df_model.transition_model.model.dit.final_layer.adaLN_modulation.1.bias: torch.Size([768]), requires_grad=True
df_model.transition_model.model.time_mlp.1.weight: torch.Size([192, 32]), requires_grad=True
df_model.transition_model.model.time_mlp.1.bias: torch.Size([192]), requires_grad=True
df_model.transition_model.model.time_mlp.3.weight: torch.Size([192, 192]), requires_grad=True
df_model.transition_model.model.time_mlp.3.bias: torch.Size([192]), requires_grad=True
df_model.transition_model.model.external_cond_emb.0.weight: torch.Size([46, 192]), requires_grad=True
df_model.transition_model.model.final_conv.weight: torch.Size([32, 36, 1, 1]), requires_grad=True
df_model.transition_model.model.final_conv.bias: torch.Size([32]), requires_grad=True
df_model.transition_model.model.tanh_layer.0.weight: torch.Size([36, 36, 1, 1]), requires_grad=True
df_model.transition_model.model.tanh_layer.0.bias: torch.Size([36]), requires_grad=True
df_model.transition_model.x_from_z.0.conv1.weight: torch.Size([4, 32, 3, 3]), requires_grad=True
df_model.transition_model.x_from_z.0.conv2.weight: torch.Size([4, 4, 3, 3]), requires_grad=True
df_model.transition_model.x_from_z.0.shortcut.0.weight: torch.Size([4, 32, 1, 1]), requires_grad=True
df_model.transition_model.x_from_z.1.weight: torch.Size([4, 4, 1, 1]), requires_grad=True
df_model.transition_model.x_from_z.1.bias: torch.Size([4]), requires_grad=True
vae.encoder.conv_in.weight: torch.Size([128, 3, 3, 3]), requires_grad=False
vae.encoder.conv_in.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.0.norm1.weight: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.0.norm1.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.0.conv1.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.encoder.down.0.block.0.conv1.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.0.norm2.weight: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.0.norm2.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.0.conv2.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.encoder.down.0.block.0.conv2.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.1.norm1.weight: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.1.norm1.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.1.conv1.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.encoder.down.0.block.1.conv1.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.1.norm2.weight: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.1.norm2.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.block.1.conv2.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.encoder.down.0.block.1.conv2.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.0.downsample.conv.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.encoder.down.0.downsample.conv.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.1.block.0.norm1.weight: torch.Size([128]), requires_grad=False
vae.encoder.down.1.block.0.norm1.bias: torch.Size([128]), requires_grad=False
vae.encoder.down.1.block.0.conv1.weight: torch.Size([256, 128, 3, 3]), requires_grad=False
vae.encoder.down.1.block.0.conv1.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.0.norm2.weight: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.0.norm2.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.0.conv2.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.encoder.down.1.block.0.conv2.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.0.nin_shortcut.weight: torch.Size([256, 128, 1, 1]), requires_grad=False
vae.encoder.down.1.block.0.nin_shortcut.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.1.norm1.weight: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.1.norm1.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.1.conv1.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.encoder.down.1.block.1.conv1.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.1.norm2.weight: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.1.norm2.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.1.block.1.conv2.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.encoder.down.1.block.1.conv2.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.1.downsample.conv.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.encoder.down.1.downsample.conv.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.2.block.0.norm1.weight: torch.Size([256]), requires_grad=False
vae.encoder.down.2.block.0.norm1.bias: torch.Size([256]), requires_grad=False
vae.encoder.down.2.block.0.conv1.weight: torch.Size([512, 256, 3, 3]), requires_grad=False
vae.encoder.down.2.block.0.conv1.bias: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.0.norm2.weight: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.0.norm2.bias: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.0.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.encoder.down.2.block.0.conv2.bias: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.0.nin_shortcut.weight: torch.Size([512, 256, 1, 1]), requires_grad=False
vae.encoder.down.2.block.0.nin_shortcut.bias: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.1.norm1.weight: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.1.norm1.bias: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.1.conv1.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.encoder.down.2.block.1.conv1.bias: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.1.norm2.weight: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.1.norm2.bias: torch.Size([512]), requires_grad=False
vae.encoder.down.2.block.1.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.encoder.down.2.block.1.conv2.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_1.norm1.weight: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_1.norm1.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_1.conv1.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.encoder.mid.block_1.conv1.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_1.norm2.weight: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_1.norm2.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_1.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.encoder.mid.block_1.conv2.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.attn_1.norm.weight: torch.Size([512]), requires_grad=False
vae.encoder.mid.attn_1.norm.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.attn_1.q.weight: torch.Size([512, 512, 1, 1]), requires_grad=False
vae.encoder.mid.attn_1.q.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.attn_1.k.weight: torch.Size([512, 512, 1, 1]), requires_grad=False
vae.encoder.mid.attn_1.k.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.attn_1.v.weight: torch.Size([512, 512, 1, 1]), requires_grad=False
vae.encoder.mid.attn_1.v.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.attn_1.proj_out.weight: torch.Size([512, 512, 1, 1]), requires_grad=False
vae.encoder.mid.attn_1.proj_out.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_2.norm1.weight: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_2.norm1.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_2.conv1.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.encoder.mid.block_2.conv1.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_2.norm2.weight: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_2.norm2.bias: torch.Size([512]), requires_grad=False
vae.encoder.mid.block_2.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.encoder.mid.block_2.conv2.bias: torch.Size([512]), requires_grad=False
vae.encoder.norm_out.weight: torch.Size([512]), requires_grad=False
vae.encoder.norm_out.bias: torch.Size([512]), requires_grad=False
vae.encoder.conv_out.weight: torch.Size([8, 512, 3, 3]), requires_grad=False
vae.encoder.conv_out.bias: torch.Size([8]), requires_grad=False
vae.decoder.conv_in.weight: torch.Size([512, 4, 3, 3]), requires_grad=False
vae.decoder.conv_in.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_1.norm1.weight: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_1.norm1.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_1.conv1.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.mid.block_1.conv1.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_1.norm2.weight: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_1.norm2.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_1.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.mid.block_1.conv2.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.attn_1.norm.weight: torch.Size([512]), requires_grad=False
vae.decoder.mid.attn_1.norm.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.attn_1.q.weight: torch.Size([512, 512, 1, 1]), requires_grad=False
vae.decoder.mid.attn_1.q.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.attn_1.k.weight: torch.Size([512, 512, 1, 1]), requires_grad=False
vae.decoder.mid.attn_1.k.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.attn_1.v.weight: torch.Size([512, 512, 1, 1]), requires_grad=False
vae.decoder.mid.attn_1.v.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.attn_1.proj_out.weight: torch.Size([512, 512, 1, 1]), requires_grad=False
vae.decoder.mid.attn_1.proj_out.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_2.norm1.weight: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_2.norm1.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_2.conv1.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.mid.block_2.conv1.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_2.norm2.weight: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_2.norm2.bias: torch.Size([512]), requires_grad=False
vae.decoder.mid.block_2.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.mid.block_2.conv2.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.0.block.0.norm1.weight: torch.Size([256]), requires_grad=False
vae.decoder.up.0.block.0.norm1.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.0.block.0.conv1.weight: torch.Size([128, 256, 3, 3]), requires_grad=False
vae.decoder.up.0.block.0.conv1.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.0.norm2.weight: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.0.norm2.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.0.conv2.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.decoder.up.0.block.0.conv2.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.0.nin_shortcut.weight: torch.Size([128, 256, 1, 1]), requires_grad=False
vae.decoder.up.0.block.0.nin_shortcut.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.1.norm1.weight: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.1.norm1.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.1.conv1.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.decoder.up.0.block.1.conv1.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.1.norm2.weight: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.1.norm2.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.1.conv2.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.decoder.up.0.block.1.conv2.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.2.norm1.weight: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.2.norm1.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.2.conv1.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.decoder.up.0.block.2.conv1.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.2.norm2.weight: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.2.norm2.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.0.block.2.conv2.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.decoder.up.0.block.2.conv2.bias: torch.Size([128]), requires_grad=False
vae.decoder.up.1.block.0.norm1.weight: torch.Size([512]), requires_grad=False
vae.decoder.up.1.block.0.norm1.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.1.block.0.conv1.weight: torch.Size([256, 512, 3, 3]), requires_grad=False
vae.decoder.up.1.block.0.conv1.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.0.norm2.weight: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.0.norm2.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.0.conv2.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.decoder.up.1.block.0.conv2.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.0.nin_shortcut.weight: torch.Size([256, 512, 1, 1]), requires_grad=False
vae.decoder.up.1.block.0.nin_shortcut.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.1.norm1.weight: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.1.norm1.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.1.conv1.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.decoder.up.1.block.1.conv1.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.1.norm2.weight: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.1.norm2.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.1.conv2.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.decoder.up.1.block.1.conv2.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.2.norm1.weight: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.2.norm1.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.2.conv1.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.decoder.up.1.block.2.conv1.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.2.norm2.weight: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.2.norm2.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.block.2.conv2.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.decoder.up.1.block.2.conv2.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.1.upsample.conv.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.decoder.up.1.upsample.conv.bias: torch.Size([256]), requires_grad=False
vae.decoder.up.2.block.0.norm1.weight: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.0.norm1.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.0.conv1.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.up.2.block.0.conv1.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.0.norm2.weight: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.0.norm2.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.0.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.up.2.block.0.conv2.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.1.norm1.weight: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.1.norm1.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.1.conv1.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.up.2.block.1.conv1.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.1.norm2.weight: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.1.norm2.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.1.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.up.2.block.1.conv2.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.2.norm1.weight: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.2.norm1.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.2.conv1.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.up.2.block.2.conv1.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.2.norm2.weight: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.2.norm2.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.block.2.conv2.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.up.2.block.2.conv2.bias: torch.Size([512]), requires_grad=False
vae.decoder.up.2.upsample.conv.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.decoder.up.2.upsample.conv.bias: torch.Size([512]), requires_grad=False
vae.decoder.norm_out.weight: torch.Size([128]), requires_grad=False
vae.decoder.norm_out.bias: torch.Size([128]), requires_grad=False
vae.decoder.conv_out.weight: torch.Size([3, 128, 3, 3]), requires_grad=False
vae.decoder.conv_out.bias: torch.Size([3]), requires_grad=False
vae.loss.logvar: torch.Size([]), requires_grad=False
vae.loss.perceptual_loss.net.slice1.0.weight: torch.Size([64, 3, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice1.0.bias: torch.Size([64]), requires_grad=False
vae.loss.perceptual_loss.net.slice1.2.weight: torch.Size([64, 64, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice1.2.bias: torch.Size([64]), requires_grad=False
vae.loss.perceptual_loss.net.slice2.5.weight: torch.Size([128, 64, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice2.5.bias: torch.Size([128]), requires_grad=False
vae.loss.perceptual_loss.net.slice2.7.weight: torch.Size([128, 128, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice2.7.bias: torch.Size([128]), requires_grad=False
vae.loss.perceptual_loss.net.slice3.10.weight: torch.Size([256, 128, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice3.10.bias: torch.Size([256]), requires_grad=False
vae.loss.perceptual_loss.net.slice3.12.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice3.12.bias: torch.Size([256]), requires_grad=False
vae.loss.perceptual_loss.net.slice3.14.weight: torch.Size([256, 256, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice3.14.bias: torch.Size([256]), requires_grad=False
vae.loss.perceptual_loss.net.slice4.17.weight: torch.Size([512, 256, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice4.17.bias: torch.Size([512]), requires_grad=False
vae.loss.perceptual_loss.net.slice4.19.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice4.19.bias: torch.Size([512]), requires_grad=False
vae.loss.perceptual_loss.net.slice4.21.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice4.21.bias: torch.Size([512]), requires_grad=False
vae.loss.perceptual_loss.net.slice5.24.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice5.24.bias: torch.Size([512]), requires_grad=False
vae.loss.perceptual_loss.net.slice5.26.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice5.26.bias: torch.Size([512]), requires_grad=False
vae.loss.perceptual_loss.net.slice5.28.weight: torch.Size([512, 512, 3, 3]), requires_grad=False
vae.loss.perceptual_loss.net.slice5.28.bias: torch.Size([512]), requires_grad=False
vae.loss.perceptual_loss.lin0.model.1.weight: torch.Size([1, 64, 1, 1]), requires_grad=False
vae.loss.perceptual_loss.lin1.model.1.weight: torch.Size([1, 128, 1, 1]), requires_grad=False
vae.loss.perceptual_loss.lin2.model.1.weight: torch.Size([1, 256, 1, 1]), requires_grad=False
vae.loss.perceptual_loss.lin3.model.1.weight: torch.Size([1, 512, 1, 1]), requires_grad=False
vae.loss.perceptual_loss.lin4.model.1.weight: torch.Size([1, 512, 1, 1]), requires_grad=False
vae.loss.discriminator.main.0.weight: torch.Size([64, 3, 4, 4]), requires_grad=False
vae.loss.discriminator.main.0.bias: torch.Size([64]), requires_grad=False
vae.loss.discriminator.main.2.weight: torch.Size([128, 64, 4, 4]), requires_grad=False
vae.loss.discriminator.main.3.weight: torch.Size([128]), requires_grad=False
vae.loss.discriminator.main.3.bias: torch.Size([128]), requires_grad=False
vae.loss.discriminator.main.5.weight: torch.Size([256, 128, 4, 4]), requires_grad=False
vae.loss.discriminator.main.6.weight: torch.Size([256]), requires_grad=False
vae.loss.discriminator.main.6.bias: torch.Size([256]), requires_grad=False
vae.loss.discriminator.main.8.weight: torch.Size([512, 256, 4, 4]), requires_grad=False
vae.loss.discriminator.main.9.weight: torch.Size([512]), requires_grad=False
vae.loss.discriminator.main.9.bias: torch.Size([512]), requires_grad=False
vae.loss.discriminator.main.11.weight: torch.Size([1, 512, 4, 4]), requires_grad=False
vae.loss.discriminator.main.11.bias: torch.Size([1]), requires_grad=False
vae.quant_conv.weight: torch.Size([8, 8, 1, 1]), requires_grad=False
vae.quant_conv.bias: torch.Size([8]), requires_grad=False
vae.post_quant_conv.weight: torch.Size([4, 4, 1, 1]), requires_grad=False
vae.post_quant_conv.bias: torch.Size([4]), requires_grad=False
   model device: cuda:0
   Diffusion parameters number: 32415368
   VAE parameters number: 72807721 (frozen)